{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e6685771"
   },
   "source": [
    "# Text Summarization  using [HuggingFace Transformers](https://huggingface.co/models?pipeline_tag=translation&sort=trending)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4c7109da"
   },
   "source": [
    "- Let's install the following librarires\n",
    "```\n",
    "    !pip install transformers\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "height": 47,
    "id": "782af222-1bea-449a-8dd4-655ad7a7b8ea"
   },
   "outputs": [],
   "source": [
    "# Code to ignore warnings\n",
    "from transformers.utils import logging\n",
    "logging.set_verbosity_error()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bea43ec1"
   },
   "source": [
    "## Let's build the translation pipeline using **Transformers** Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "height": 47,
    "id": "d1d46ac9-d665-4690-99a4-43b625e02114"
   },
   "outputs": [],
   "source": [
    "# Code 1 - Use a pipeline as a high-level helper\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RR6RN1-oOgDq"
   },
   "source": [
    " The process:\n",
    "\n",
    "1. Model Download: When you create a pipeline, the Transformers library automatically downloads the model weights and configuration files to your local machine. By default, these are stored in a cache directory (usually ~/.cache/huggingface/transformers/).\n",
    "   \n",
    "2. Local Execution: Once downloaded, the model runs entirely on your local machine. This means you don't need an internet connection after the initial download, and you're not making API calls to Hugging Face servers for inference.\n",
    "\n",
    "3. First-time vs. Subsequent Use: The first time you use a specific model with a pipeline, it will download the necessary files. On subsequent runs, it will use the locally cached version unless you explicitly request a new download.\n",
    "\n",
    "4. Resource Usage: Since the model runs locally, it uses your system's resources (CPU, GPU, RAM) for computations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RthaBxAMdC5A",
    "outputId": "df88c56c-8c5e-4162-de3f-1eabdbdcd8ed"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Code 2 - create a summarizer object\n",
    "summarizer = pipeline(task=\"summarization\", model=\"facebook/bart-large-cnn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "8WP19UcpNEBw"
   },
   "outputs": [],
   "source": [
    "text = \"\"\"BART is a transformer encoder-encoder (seq2seq) model with a bidirectional (BERT-like) encoder and an autoregressive (GPT-like) decoder.\n",
    "BART is pre-trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text.\n",
    "BART is particularly effective when fine-tuned for text generation (e.g. summarization, translation) but also works well for comprehension tasks (e.g. text classification, question answering).\n",
    "This particular checkpoint has been fine-tuned on CNN Daily Mail, a large collection of text-summary pairs.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "IFB9-_XgdKpF"
   },
   "outputs": [],
   "source": [
    "# Summarize the text. the length here is in tokens\n",
    "summary = summarizer(text, min_length=10, max_length=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_FumQd2qdMky",
    "outputId": "21f97208-75ed-4292-a529-e7db6c42bea5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'summary_text': 'BART is a transformer encoder-encoder (seq2seq) model with a bidirectional (BERT-like) encoder. BART is pre-trained by corrupting text with an arbitrary noising function, and learning a model to reconstruct the original text.'}]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print the summary\n",
    "summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NPlGtnVJRMEN"
   },
   "source": [
    "#### To improve the quality of your summary, try the following"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "nrQ1c4fLQKVF"
   },
   "outputs": [],
   "source": [
    "# Code 3 - Summarize the text. the length here is in tokens\n",
    "summary = summarizer(text,\n",
    "    repetition_penalty=5.0,  # Increase this to discourage repetition\n",
    "    length_penalty=0.3,      # Decrease this to generate longer summaries\n",
    "    min_length=20, max_length=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Zel5kB8OQhXR",
    "outputId": "75f69280-6bb0-49d8-8f7f-14ed420f792f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'summary_text': 'BART is pre-trained by corrupting text with an arbitrary noising function, and learning a model to reconstruct the original text. BART is particularly effective when fine-tuned for text generation but also works well for comprehension tasks.'}]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bUaHxNDXSNlG"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r4s6sB9JSNcV"
   },
   "source": [
    "## Create an Inference point"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradio is an open-source Python library that provides an easy way to create web-based user interfaces for machine learning models, data processing pipelines, or any Python function. Here are the key points about Gradio:\n",
    "\n",
    "* It allows developers to quickly create interactive demos for their machine learning models or data science projects without needing expertise in web development.\n",
    "* Gradio supports various input types (text, image, audio, video) and output types, making it suitable for a wide range of applications.\n",
    "* It works well with popular machine learning frameworks like TensorFlow, PyTorch, and scikit-learn, as well as with Hugging Face models.\n",
    "* You can run Gradio interfaces locally or easily deploy them to sharing services like Hugging Face Spaces.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Bi38pRN5VIma"
   },
   "outputs": [],
   "source": [
    "!pip install gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "PO31N62NVFAT"
   },
   "outputs": [],
   "source": [
    "# Code 4 - Let's create an UI using Gradio\n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "G5_tio7lVV-y"
   },
   "outputs": [],
   "source": [
    "# Code 5 - define a function to summarize text\n",
    "def nlp(input_text):\n",
    "    summary = summarizer(\n",
    "        input_text,\n",
    "        repetition_penalty=5.0,  # Increase this to discourage repetition\n",
    "        length_penalty=0.3,      # Decrease this to generate longer summaries\n",
    "        min_length=20, max_length=100\n",
    "    )\n",
    "    return summary[0][\"summary_text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "zN5XBXt8WMhw"
   },
   "outputs": [],
   "source": [
    "# Code 6 - UI object\n",
    "ui = gr.Interface(nlp,\n",
    "    inputs=gr.Textbox(label=\"Input Text\"),\n",
    "    outputs=gr.Textbox(label=\"Summary\"),\n",
    "    title=\"Text Summarizer\",\n",
    "    description=\"Summarize your text using the BART model.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 643
    },
    "id": "_ouGPdaGXfQy",
    "outputId": "ad85c7a6-1371-4454-9b99-aa520272d5a6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting queue=True in a Colab notebook requires sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
      "\n",
      "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
      "Running on public URL: https://92207fcaf235bb4757.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://92207fcaf235bb4757.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Code 7 - launch UI\n",
    "ui.launch(share=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ytl7H_KYSNL1"
   },
   "source": [
    "# Model Deployement using HuggingFace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jBN6eEzySShx"
   },
   "source": [
    "step 1: Visit [HugginFace.co ](https://https://huggingface.co/) \\\n",
    "step 2: Find the New button (left side of the page) and create a new space. \\\n",
    "step 3: space name = Bart-Text-Summarizer, license = Apache 2.0, space sdk = Gradio, Space hardware = Free type, and keep the public option selected.\\\n",
    "step 4: Once the space is created, click on files and create 2 files, requirement.txt (with all the required libraries) and app.py (application file)\\\n",
    "step 5: Inside your requirements.txt file, list - transformers and gradio. \\\n",
    "step 6: create another file app.py, and add the code from code 1, code 2, code 5, code 6, and code 7\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2Bpv7cgLk92c"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-sd8kCh0k9rZ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YrxaxRHaSRaS"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
